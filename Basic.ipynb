{
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<h1 style=\"text-align: center;\"> Linear Regression With Tensorflow Sharing Session </h1>\n<center><img src=\"https://res-2.cloudinary.com/crunchbase-production/image/upload/c_lpad,h_256,w_256,f_auto,q_auto:eco/qbient9azpblv8bekjhj\"  align=\"middle\"></center>\n"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "> There’s a huge difference between building a Jupyter notebook model in the lab and deploying a production system that generates business value. AI as a field sometimes seems crowded but, in fact, it’s wide open to professionals who know what they’re doing.\n\nAndrew Ng - The Batch"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "Tensorfow is confusing, using it and customize it requires deep understanding on Machine Learning theory"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef build_model():\n    model = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n    ])\n    \n    optimizer = tf.keras.optimizers.RMSprop(0.001)\n    \n    model.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])\n    \n    return model\n\nmodel = build_model()\nmodel.fit(normed_train_data, train_labels)\nmodel.evaluate(normed_test_data, test_labels)\nmodel.predict()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Objective\n\n1. Learn basic linear regression with one variable\n2. Write from sracth using python\n3. Rewrite it back using tensorflow 2.0\n4. pray that everything just work"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Scalars, Vectors, Matrices and Tensors\n\n**Scalar:** is just a single number\n\n\\begin{equation*}\ns \\in \\mathbb{R} \n\\end{equation*}\n**Vector:** A vector is an array of numbers.\n\n\\begin{equation*}\nx =  \\left[\n\\begin{matrix}\n x_1 \\\\\n x_2 \\\\\n \\vdots \\\\\nx_n \\\\ \n\\end{matrix}\n\\right]\n\\end{equation*}\n\n**Matrices:** A matrix is a 2-D array of numbers\n\n\\begin{equation*}\nA = \\begin{bmatrix}\nA_1,_1 & A_1,_2 & A_1,_3\\\\ \nA_2,_1 & A_2,_2 & A_2,_3\n\\end{bmatrix}\n\\end{equation*}\n"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Use numpy",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": " ## Supervised And Unsupervised\n \n**Supervised Learning** : In supervised learning, we are given a data set and already know what our correct output should look like, having the idea\nthat there is a relationship between the input and the output.\n\n**Unsupervised learning** : on the other hand, allows us to approach problems with little or no idea what our results should\nlook like. We can derive structure from data where we don't necessarily know the e􀃗ect of the variable\n\n<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRc1XTSwQBPRwCB7RGGb_F1zVorW3dDjFW5SF2em3EiAlizLvwt&s\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Supervised Machine Learning"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "## Imports time\nLets import all Library first"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Random Data Generator"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "def generate_data():\n    grad = np.random.uniform(low=1,high=3,size=1)[0]\n    bias = np.random.uniform(low=0,high=10,size=1)[0]\n    f = lambda x,grad,bias,penality,logit: grad*x+bias+(penality*logit)\n    logit = lambda x : 1 / (1 + np.power((x/(1-x)),2) )\n    norm = lambda x: np.abs(x - 10)/10\n    x = np.random.uniform(low=5,high=15,size=30)\n    x_logit = logit(norm(x))\n    penality = np.random.uniform(low=-3,high=3,size=30)\n\n    return x , f(x,grad,bias,penality,x_logit), grad, bias,",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "x_data, y_data, grad, bias = generate_data()\nx_data, y_data, grad, bias",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Plot function"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "def plot_data(x_data,y_data, x_pred, y_pred):\n    fig = plt.figure(figsize=(10,5)); ax = plt.axes()\n    ax.set_ylim(bottom=0,top=50); ax.set_xlim(left=0,right=20)\n    ax.plot(x_pred,y_pred)\n    ax.scatter(x_data, y_data,marker=\"x\",c=\"red\")\n    ax.set_xlabel(\"Number of year experience\")\n    ax.set_ylabel(\"Salary (RM 100's)\")\n    ax.grid()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": " plot_da",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Salary prediction  \n\ngiven $x$ is the number of year experience and, $y$ is the given salary, predict the salary for new hiring based on the working experience"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "plot_data(x_data, y_data,[0,20],[8.0, 40.0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Linear Regression With One Variable\n\n- predict a single output value y from a single input value x.\n- Hypothesis = $h(x) = \\Theta + \\Theta_1 x $\n"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "class model:\n    def __init__(self):\n        self.thetas = np.random.uniform(low=-5,high=10,size=2)\n    \n    def __call__(self,x):\n        return self.thetas[0] + self.thetas[1]*x",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Linear Regression With One Variable Cost Function\n\nThe cost function $J$ for a particular choice of parameters θ is the mean squared error (MSE):\n\n\\begin{equation*}\nJ(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{i})^2 \n\\end{equation*}\n\n\n$m$       = The number of training example  \n$x^{(i)}$ = The input vector for the $i^{th}$ training example  \n$y^{(i)}$ = The class label for the $i^{th}$ training example  \n$\\theta$  = The chosen parameter values or \"weights\" $(\\theta_0,\\theta_1,\\theta_2)$ \n"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "def MSE(y_pred,y_ground):\n    res = np.sum((np.power(y_pred - y_ground,2)))/(2*y_pred.size)\n    return res",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Gradient descent algorithm\n\nrepeat until convergence\n\n$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j } J(\\theta_0,\\theta_1)  \\qquad (for\\: j=0\\: and\\:  j=1)$"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "def gradient_descent(m: model, x_data, y_data,alpha: int):\n    '''\n    m : model\n    alpha : learning rate\n    '''\n    y_pred = m(x_data)\n    old_thetas = m.thetas\n    new_thetas = np.empty(old_thetas.size)\n    \n    new_thetas[0] = alpha * np.sum(y_pred - y_data)/(y_pred.size)\n    new_thetas[1] = alpha * np.sum((y_pred - y_data)* x_data)/(y_pred.size) \n    new_thetas = old_thetas - new_thetas\n    \n    m.thetas = new_thetas",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Combining it all"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(x_data)\nprint(\"Starting thetas : \",m.thetas)\nprint(\"Expected thetas : \",bias, \" \", grad)\nfor i in range(500):\n    if i%20 == 0:\n        print(MSE(m(x_data),y_data))\n    gradient_descent(m,x_data,y_data,0.01)\n    #print(\"loop \",i , \" thetas : \", m.thetas)\n\nprint(\"After thetas : \",m.thetas)\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<center><img src=\"https://www.gstatic.com/devrel-devsite/prod/v18acaed6c9e1965881c7cfa6f3f9f69e7c37603f6d253114168821ce9e70c645/tensorflow/images/lockup.svg\"  align=\"middle\" style=\"width: 25vw; min-width: 130px;\" ></center>\n<h1 style=\"text-align: center;\"> Version 2.0 </h1>"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Eager execution\nTensorFlow 1.X requires users to manually stitch together an abstract syntax tree (the graph) by making tf.* API calls. It then requires users to manually compile the abstract syntax tree by passing a set of output tensors and input tensors to a session.run() call. TensorFlow 2.0 executes eagerly (like Python normally does) and in 2.0, graphs and sessions should feel like implementation details."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "Lets import tensorflow"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\ntf.__version__",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Tensorflow Tensor\n\nA Tensor is a multi-dimensional array. Similar to NumPy `ndarray` objects, `tf.Tensor` objects have a data type and a shape. Additionally, `tf.Tensors` can reside in accelerator memory (like a GPU). TensorFlow offers a rich library of operations (`tf.add, tf.matmul, tf.linalg.inv` etc.) that consume and produce `tf.Tensor`s. These operations automatically convert native Python types"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(tf.add(1, 2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\n\nndarray = np.ones([3, 3])\n\nprint(\"TensorFlow operations convert numpy arrays to Tensors automatically\")\ntensor = tf.multiply(ndarray, 42)\nprint(tensor)\n\n\nprint(\"And NumPy operations convert Tensors to numpy arrays automatically\")\nprint(np.add(tensor, 1))\n\nprint(\"The .numpy() method explicitly converts a Tensor to a numpy array\")\nprint(tensor.numpy())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## GPU acceleration\n\nMany TensorFlow operations are accelerated using the GPU for computation. Without any annotations, TensorFlow automatically decides whether to use the GPU or CPU for an operation—copying the tensor between CPU and GPU memory, if necessary. Tensors produced by an operation are typically backed by the memory of the device on which the operation executed, for example:"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"Is there a GPU available: \"),\nprint(tf.config.experimental.list_physical_devices(\"GPU\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Keras\n\ntf.keras is TensorFlow's implementation of the Keras API specification. This is a high-level API to build and train models that includes first-class support for TensorFlow-specific functionality, such as eager execution, tf.data pipelines, and Estimators. tf.keras makes TensorFlow easier to use without sacrificing flexibility and performance."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "# Tensorflow Linear Regression"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\n\ntf.keras.backend.clear_session()  # For easy reset of notebook state.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## The Layer class\n\n### Layers encapsulate a state (weights) and some computation\n\nThe main data structure you'll work with is the Layer. A layer encapsulates both a state (the layer's \"weights\") and a transformation from inputs to outputs (a \"call\", the layer's forward pass).\n\nHere's a densely-connected layer. It has a state: the variables w and b."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "from tensorflow.keras import layers\n\n\nclass Linear(layers.Layer):\n\n    def __init__(self, units=32, input_dim=32):\n        super(Linear, self).__init__()\n        w_init = tf.random_normal_initializer()\n        self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units),\n                                              dtype='float32'),trainable=True)\n        b_init = tf.zeros_initializer()\n        self.b = tf.Variable(initial_value=b_init(shape=(units,),\n                                              dtype='float32'),trainable=True)\n\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n\nx = tf.ones((2, 2))\nlinear_layer = Linear(4, 2)\ny = linear_layer(x)\nprint(y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "Can be further simplify"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": false
      },
      "cell_type": "code",
      "source": "class Linear(layers.Layer):\n    def __init__(self, units=32, input_dim=32):\n        super(Linear, self).__init__()\n        self.w = self.add_weight(shape=(input_dim, units),\n                             initializer='random_normal',\n                             trainable=True)\n        self.b = self.add_weight(shape=(units,),\n                             initializer='zeros',\n                             trainable=True)\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n\nx = tf.ones((2, 2))\nlinear_layer = Linear(4, 2)\ny = linear_layer(x)\nprint(y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from tensorflow.keras import layers\n\nclass model(layers.Layer):\n    def __init__(self):\n        super(model, self).__init__()\n       \n    def build(self,input_shape):\n        self.w = self.add_weight(shape=(1,),\n                                 initializer=tf.initializers.RandomUniform(minval=-5,\n                                            maxval=1,\n                                            seed=None))\n        #shape=(,1)\n        self.b = self.add_weight(shape=(1,),\n                                 initializer=tf.initializers.RandomUniform(minval=-5,\n                                            maxval=1,\n                                            seed=None))\n        \n    def call(self,inputs):\n        res = tf.multiply(inputs,self.w) + self.b\n        return res\n    \n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "\nm = model()\n\n# x = tf.ones(1)\n\ny = m(x_data)\n\nprint(y)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "m.weights",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Instantiate an optimizer.\n# An optimizer is one of the two arguments required for compiling a Keras model:\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-3) # Stochastic gradient descent optimizer.\n# loss_fn = tf.keras.losses.mean_squared_error() \n#keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# Iterate over the batches of a dataset.\n# for x_batch_train, y_batch_train in train_dataset:\nfor i in range(500):\n    with tf.GradientTape() as tape:\n#     logits = layer(x_batch_train)  # Logits for this minibatch\n    # Loss value for this minibatch\n#     loss_value = loss_fn(y_batch_train, logits)\n        logits = m(x_data)\n        loss_value = tf.keras.losses.mean_squared_error(y_data, logits)\n    # Add extra losses created during this forward pass:\n#     loss_value += sum(model.losses)\n    print(loss_value)\n    grads = tape.gradient(loss_value, m.trainable_weights)\n    optimizer.apply_gradients(zip(grads, m.trainable_weights))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "m(ee)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "m.weights",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "ee = np.asarray([0.0,20.0])",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}