{
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<h1 style=\"text-align: center;\"> Linear Regression With Tensorflow Sharing Session </h1>\n<center><img src=\"https://res-2.cloudinary.com/crunchbase-production/image/upload/c_lpad,h_256,w_256,f_auto,q_auto:eco/qbient9azpblv8bekjhj\"  align=\"middle\"></center>\n"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "> There’s a huge difference between building a Jupyter notebook model in the lab and deploying a production system that generates business value. AI as a field sometimes seems crowded but, in fact, it’s wide open to professionals who know what they’re doing.\n\nAndrew Ng - The Batch"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "Tensorfow is confusing, using it and customize it requires deep understanding on Machine Learning theory"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef build_model():\n    model = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n    ])\n    \n    optimizer = tf.keras.optimizers.RMSprop(0.001)\n    \n    model.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])\n    \n    return model\n\nmodel = build_model()\nmodel.fit(normed_train_data, train_labels)\nmodel.evaluate(normed_test_data, test_labels)\nmodel.predict()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Objective\n\n1. Learn basic linear regression with one variable\n2. Write from sracth using python\n3. Rewrite it back using tensorflow 2.0\n4. pray that everything just work"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Scalars, Vectors, Matrices and Tensors\n\n**Scalar:** is just a single number\n\n\\begin{equation*}\ns \\in \\mathbb{R} \n\\end{equation*}\n**Vector:** A vector is an array of numbers.\n\n\\begin{equation*}\nx =  \\left[\n\\begin{matrix}\n x_1 \\\\\n x_2 \\\\\n \\vdots \\\\\nx_n \\\\ \n\\end{matrix}\n\\right]\n\\end{equation*}\n\n**Matrices:** A matrix is a 2-D array of numbers\n\n\\begin{equation*}\nA = \\begin{bmatrix}\nA_1,_1 & A_1,_2 & A_1,_3\\\\ \nA_2,_1 & A_2,_2 & A_2,_3\n\\end{bmatrix}\n\\end{equation*}\n\n**Tensor:** Anything that is more than 3-D array"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\n\nA = np.asarray([ [2,3], [7,8] ])\nB = np.asarray([ [1,5], [4,3] ])\nA.dot(B)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": " ## Supervised And Unsupervised\n \n**Supervised Learning** : In supervised learning, we are given a data set and already know what our correct output should look like, having the idea\nthat there is a relationship between the input and the output.\n\n**Unsupervised learning** : on the other hand, allows us to approach problems with little or no idea what our results should\nlook like. We can derive structure from data where we don't necessarily know the e􀃗ect of the variable\n\n<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRc1XTSwQBPRwCB7RGGb_F1zVorW3dDjFW5SF2em3EiAlizLvwt&s\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Supervised Machine Learning"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "markdown",
      "source": "## Imports time\nLets import all Library first"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tabulate import tabulate\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Salary prediction  \n\ngiven $x$ is the number of year experience and, $y$ is the given salary, predict the salary for new hiring based on the working experience"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Random Data Generator"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "def generate_data():\n    grad = np.random.uniform(low=1,high=3,size=1)[0].astype('float32')\n    bias = np.random.uniform(low=0,high=10,size=1)[0].astype('float32')\n    f = lambda x,grad,bias,penality,logit: grad*x+bias+(penality*logit)\n    logit = lambda x : 1 / (1 + np.power((x/(1-x)),2) )\n    norm = lambda x: np.abs(x - 10)/10\n    x = np.random.uniform(low=5,high=15,size=30)\n    x_logit = logit(norm(x))\n    penality = np.random.uniform(low=-3,high=3,size=30)\n\n    return x.astype('float32') , f(x,grad,bias,penality,x_logit).astype('float32')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "x_data, y_data,  = generate_data()\nprint(tabulate({\"x\":x_data, \"y\":y_data}, headers=['x_data','y_data']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Plot function"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "def plot_data(x_data,y_data, x_pred, y_pred):\n    fig = plt.figure(figsize=(10,5)); ax = plt.axes()\n    ax.set_ylim(bottom=0,top=50); ax.set_xlim(left=0,right=20)\n    ax.plot(x_pred,y_pred)\n    ax.scatter(x_data, y_data,marker=\"x\",c=\"red\")\n    ax.set_xlabel(\"Number of year experience\")\n    ax.set_ylabel(\"Salary (RM 100's)\")\n    ax.grid()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "plot_data(x_data, y_data,[0,20],[2.5,24])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Model representation\n\n<img src=\"https://i.ibb.co/DtMbc36/train.png\" alt=\"train\" border=\"0\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Linear Regression With One Variable\n\n- predict a single output value y from a single input value x.\n- Hypothesis = $h(x) = \\Theta_0 + \\Theta_1 x $\n- $\\Theta $ are initiated with random number"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a class name Model that implement h(x)\nclass Model:\n    def __init__(self): # Constructor\n        self.thetas = np.random.uniform(low=-5,high=10, size=2)\n    \n    def __call__(self, x):\n        return self.thetas[0] + self.thetas[1] * x\n\n# Create an instance of Model called m\nm = Model()\nm(x_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Linear Regression With One Variable Cost Function\n\nThe cost function $J$ for a particular choice of parameters θ is the mean squared error (MSE):\n\n\\begin{equation*}\nJ(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{i})^2 \n\\end{equation*}\n\n\n$m$       = The number of training example  \n$x^{(i)}$ = The input vector for the $i^{th}$ training example  \n$y^{(i)}$ = The class label for the $i^{th}$ training example  \n$\\theta$  = The chosen parameter values or \"weights\" $(\\theta_0,\\theta_1,\\theta_2)$ \n\nThe goal is to **MINIMIZE** $ \\qquad J(\\theta) \\approx 0$\n"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Implement Mean Sqaure Error here\ndef MSE(y_pred,y_ground):\n    res = np.sum((np.power(y_pred - y_ground,2)))/(2*y_pred.size)\n    return res",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Test MSE with model\nMSE(m(x_data), y_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Gradient descent algorithm\n\nrepeat until convergence  \n$ \\qquad \\{ $  \n$ \\qquad \\qquad \\theta_0 := \\theta_0 - \\alpha \\frac{\\partial}{\\partial\\theta_0 } J(\\theta_0,\\theta_1)  $  \n$ \\qquad \\qquad \\theta_1 := \\theta_1 - \\alpha \\frac{\\partial}{\\partial\\theta_1 } J(\\theta_0,\\theta_1)  $  \n$ \\qquad \\} $  \n  \nwhere :  \n$ \\alpha $ = learning rate  \n$ J(\\theta_0,\\theta_1) $ = cost function "
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Gradient descent algorithm (Expand)\nrepeat until convergence  \n$ \\qquad \\{ $  \n$ \\qquad \\qquad \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{i})  $  \n$ \\qquad \\qquad \\theta_1 := \\theta_1 - \\alpha  \\frac{1}{m} \\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{i}) \\cdot x^i $  \n$ \\qquad \\} $  "
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "def gradient_descent(m: Model, x_data, y_data, alpha: int):\n    '''\n    m : model\n    alpha : learning rate\n    '''\n    y_pred = m(x_data)\n    old_thetas = m.thetas\n    new_thetas = np.empty(old_thetas.size)\n    \n    new_thetas[0] = alpha * np.sum(y_pred - y_data)/(y_pred.size)\n    new_thetas[1] = alpha * np.sum((y_pred - y_data)* x_data)/(y_pred.size) \n    new_thetas = old_thetas - new_thetas\n    \n    m.thetas = new_thetas ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Combining it all"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Starting thetas : \",m.thetas)\n\nfor i in range(500):\n    if i%20 == 0:\n        print(MSE(m(x_data),y_data))\n    gradient_descent(m,x_data,y_data,0.01)\n\nprint(\"After thetas : \",m.thetas)\n# m.thetas\n# plot_data(x_data,y_data,[0,20],m(np.asarray([0,20])))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "plot_data(x_data,y_data,[0,20],m(np.asarray([0,20])))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<center><img src=\"https://www.gstatic.com/devrel-devsite/prod/v18acaed6c9e1965881c7cfa6f3f9f69e7c37603f6d253114168821ce9e70c645/tensorflow/images/lockup.svg\"  align=\"middle\" style=\"width: 25vw; min-width: 130px;\" ></center>\n<h1 style=\"text-align: center;\"> Version 2.0 </h1>"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Eager execution\nTensorFlow 1.X requires users to manually stitch together an abstract syntax tree (the graph) by making tf.* API calls. It then requires users to manually compile the abstract syntax tree by passing a set of output tensors and input tensors to a session.run() call. TensorFlow 2.0 executes eagerly (like Python normally does) and in 2.0, graphs and sessions should feel like implementation details."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "Lets import tensorflow"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\ntf.__version__",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Tensorflow Tensor\n\nA Tensor is a multi-dimensional array. Similar to NumPy `ndarray` objects, `tf.Tensor` objects have a data type and a shape. Additionally, `tf.Tensors` can reside in accelerator memory (like a GPU). TensorFlow offers a rich library of operations (`tf.add, tf.matmul, tf.linalg.inv` etc.) that consume and produce `tf.Tensor`s. These operations automatically convert native Python types"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "val = np.asarray([10,20])\n\n# tf.Variable\ntfvar = tf.Variable(val)\n\n# tf.constant\ntfcon = tf.constant(val)\n\n# tf.ones\ntfone = tf.ones((2,))\n\nval, tfvar,tfcon, tfone",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "code",
      "source": "# Exercise Theta0 + Theta1 * x\n# use tf.multiply and tf.add()\nthetas = tf.Variable(np.random.uniform(low=-5,high=10, size=(2,)))\ntf.add(tf.multiply(x_data,thetas[1]), thetas[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## GPU acceleration\n\nMany TensorFlow operations are accelerated using the GPU for computation. Without any annotations, TensorFlow automatically decides whether to use the GPU or CPU for an operation—copying the tensor between CPU and GPU memory, if necessary. Tensors produced by an operation are typically backed by the memory of the device on which the operation executed, for example:"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Is there a GPU available: \"),\nprint(tf.config.experimental.list_physical_devices(\"GPU\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Keras\n\ntf.keras is TensorFlow's implementation of the Keras API specification. This is a high-level API to build and train models that includes first-class support for TensorFlow-specific functionality, such as eager execution, tf.data pipelines, and Estimators. tf.keras makes TensorFlow easier to use without sacrificing flexibility and performance."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "# Tensorflow Linear Regression"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\n\ntf.keras.backend.clear_session()  # For easy reset of notebook state.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## The Layer class\n\n### Layers encapsulate a state (weights) and some computation\n\nThe main data structure you'll work with is the Layer. A layer encapsulates both a state (the layer's \"weights\") and a transformation from inputs to outputs (a \"call\", the layer's forward pass).\n"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Add tensorflow.keras.layers\nfrom tensorflow.keras import layers\n\n# create class name Model which inherit layers.layer\nclass ModelTF(layers.Layer):\n    def __init__(self):\n        super(ModelTF,self).__init__()\n        self.thetas = self.add_weight(shape=(2,),\n                                     initializer=tf.initializers.RandomUniform(minval=-5,maxval=10,seed=None),\n                                     dtype='float32',\n                                     trainable=True)\n    def call(self, inputs):\n        return tf.add(tf.multiply(inputs, self.thetas[0]), self.thetas[1])\n    \n# Create an instance of Model\nm = ModelTF()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "Lets plot original weights before training"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "plot_data(x_data,y_data,[0,20],m(np.asarray([0,20]).astype('float32')))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "cell_type": "markdown",
      "source": "## Using the GradientTape: Auto differiantial  \nCalling a model inside a GradientTape scope enables you to retrieve the gradients of the trainable weights of the layer with respect to a loss value. Using an optimizer instance, you can use these gradients to update these variables (which you can retrieve using `model.trainable_weights`)."
    },
    {
      "metadata": {
        "trusted": true,
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "code",
      "source": "# Instantiate an optimizer.\n# An optimizer is one of the two arguments required for compiling a Keras model:\noptimizer = tf.keras.optimizers.SGD(learning_rate=5e-3) # Stochastic gradient descent optimizer.\n  \nfor i in range(500):\n    \n    with tf.GradientTape() as tape:\n        logits = m(x_data)\n        loss_value = tf.keras.losses.mean_squared_error(y_data, logits)\n\n    print(loss_value)\n    grads = tape.gradient(loss_value, m.trainable_weights)\n    optimizer.apply_gradients(zip(grads, m.trainable_weights))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "plot_data(x_data,y_data,[0,20],m(np.asarray([0,20]).astype('float32')))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}